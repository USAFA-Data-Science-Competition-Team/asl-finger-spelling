{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "data_path = 'C:/Users/C25Thomas.Blalock/Coding/Data Competition Team/asl/codebase/asl-finger-spelling/development_data'\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dataset class to load images\n",
    "class FingerspellingDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Loop through data directory and read in images and labels\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                image_path = os.path.join(root, file)\n",
    "                label = os.path.basename(root)\n",
    "                \n",
    "                image = Image.open(image_path) \n",
    "                image = image.convert('RGB') # makes channels first dim\n",
    "                image = transforms.ToTensor()(image) \n",
    "                self.images.append(image)\n",
    "                self.labels.append(label)\n",
    "\n",
    "        # Create label maps\n",
    "        self.label_map = {}\n",
    "        self.label_text = []\n",
    "\n",
    "        for label in self.labels:\n",
    "            if label not in self.label_map:\n",
    "                self.label_map[label] = len(self.label_map)\n",
    "                self.label_text.append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label_text = self.labels[idx]\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label_idx = self.label_map[label_text]\n",
    "        label_tensor = torch.tensor(label_idx)\n",
    "        \n",
    "        return image, label_tensor, label_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix sizing for convolutional layers: https://www.baeldung.com/cs/convolutional-layer-size\n",
    "\n",
    "Sizing calculator: https://madebyollin.github.io/convnet-calculator/\n",
    "\n",
    "out_channels is filter count in the calculator\n",
    "\n",
    "spacial extent is kernal_size\n",
    "\n",
    "Input: Batch_size = N ; Number_of_Channels = C_in ; Height = H_in ; Width = W_in\n",
    "\n",
    "Input: num_channels x heigt x width x Batch_Size\n",
    "\n",
    "Conv Layers Params: in_channels = C_in ; out_channels = C_out ; kernel_size = K ; stride = S ; padding = P\n",
    "\n",
    "Output of Conv Layer: Batch_size = N ; Number_of_Channels = C_out ; Height = H_out ; Width = W_out\n",
    "\n",
    "Output of Conv Layer: N x C_out x H_out x W_out\n",
    "\n",
    "H_out = (H_in + 2*P - K)/S + 1\n",
    "\n",
    "W_out = (W_in + 2*P - K)/S + 1\n",
    "\n",
    "ReLu: size_in = size_out\n",
    "\n",
    "MaxPool Params: kernel_size = K ; stride = S\n",
    "\n",
    "Output of MaxPool: Batch_size = N ; Number_of_Channels = C_out ; Height = H_out ; Width = W_out\n",
    "\n",
    "H_out = (H_in - K)/S + 1\n",
    "\n",
    "W_out = (W_in - K)/S + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define network architecture \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            # add batch norm\n",
    "\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Convolutional layer 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Resize layer so dense can process\n",
    "        self.resize = nn.Sequential(\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Dense layer 1\n",
    "        self.dense1 = nn.Sequential(\n",
    "            torch.nn.Linear(in_features=64*25*25, out_features=1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.3)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            torch.nn.Linear(in_features=1024, out_features=29),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Output: 28x1xBatch_Size\n",
    "        # 28 = 26 letters + 1 space + 1 nothing + 1 del\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.conv1(x)\n",
    "        # print(\" conv1 Output Shape: \", x.shape) # conv1 Output Shape:  torch.Size([28, 16, 100, 100])\n",
    "        x = self.conv2(x)\n",
    "        # print(\" conv2 Output Shape: \", x.shape) # conv2 Output Shape:  torch.Size([28, 32, 50, 50])\n",
    "        x = self.conv3(x)\n",
    "        # print(\" conv3 Output Shape: \", x.shape) # conv3 Output Shape:  torch.Size([28, 64, 25, 25])\n",
    "        x = self.resize(x)\n",
    "        # print(\" resize Output Shape: \", x.shape) # resize Output Shape:  torch.Size([28, 40000])\n",
    "        x = self.dense1(x)\n",
    "        # print(\" dense1 Output Shape: \", x.shape) # dense1 Output Shape:  torch.Size([28, 1024])\n",
    "        x = self.output(x)\n",
    "        # print(\" output Output Shape: \", x.shape) # output Output Shape:  torch.Size([28, 29])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset and dataloader, test and train\n",
    "dataset = FingerspellingDataset(data_path)\n",
    "\n",
    " # split into a train and test set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 200, 200])\n",
      "Z 28\n",
      "I 9\n",
      "R 19\n",
      "Space 21\n",
      "V 24\n"
     ]
    }
   ],
   "source": [
    "# check to see the dataloader is working\n",
    "for image, label_tensor, label_text in train_loader:\n",
    "    print(image.shape)\n",
    "    for i in range(5):\n",
    "        print(label_text[i], label_tensor[i].item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate model, loss function and optimizer\n",
    "model = Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.3453, Accuracy: 7.1429%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m \u001b[39m# Print loss  \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels, _ in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print loss  \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {100*torch.sum(labels==torch.argmax(outputs, dim=1))/len(labels):.4f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels, _ in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(f'Accuracy: {correct/total*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
